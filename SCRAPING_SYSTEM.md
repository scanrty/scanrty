# ü§ñ SYST√àME DE SCRAPING AUTOMATIQUE - SCANRTY

## ‚úÖ CE QUI A √âT√â CR√â√â (PHASE 1 - MVP)

### **1. Service de Scraping (`/lib/scraping.ts`)**
Moteur de d√©tection qui analyse les plateformes :
- ‚úÖ Airbnb.fr
- ‚úÖ Booking.com
- ‚úÖ Abritel.fr
- ‚úÖ Le Bon Coin (locations vacances)

**Fonctionnalit√©s :**
- Recherche par adresse, ville, caract√©ristiques
- Calcul de score de similarit√© (0-100)
- Classification suspicion (low/medium/high)
- Extraction des raisons de suspicion

### **2. G√©n√©rateur de Rapports (`/lib/report.ts`)**
Cr√©ation de rapports professionnels :
- ‚úÖ Format **texte** (pour logs/emails)
- ‚úÖ Format **HTML** (pour emails clients)
- ‚úÖ Design aux couleurs ScanRty
- ‚úÖ R√©sum√© visuel des d√©tections

### **3. Webhook Automatis√© (`/app/api/webhooks/route.ts`)**
D√©clenchement automatique apr√®s paiement :
- ‚úÖ R√©cup√©ration des donn√©es du bien (metadata Stripe)
- ‚úÖ Lancement du scraping en arri√®re-plan
- ‚úÖ G√©n√©ration du rapport
- ‚úÖ Envoi par email (√† venir)

### **4. API de Checkout (`/app/api/create-checkout/route.ts`)**
Cr√©ation de sessions Stripe avec metadata :
- ‚úÖ Envoie toutes les infos du bien √† Stripe
- ‚úÖ Disponible dans le webhook
- ‚úÖ Pas besoin de base de donn√©es !

### **5. Formulaire de Commande Mis √† Jour**
- ‚úÖ Envoie les donn√©es directement √† l'API
- ‚úÖ Cr√©e une session Stripe avec metadata
- ‚úÖ Redirection automatique vers paiement

---

## üéØ COMMENT √áA FONCTIONNE MAINTENANT

### **Parcours Client :**

1. **Client remplit le formulaire** sur `/commander`
   - Infos du bien (adresse, pi√®ces, surface, photos...)
   - Infos client (nom, email, t√©l√©phone)

2. **Clique "Proc√©der au paiement"**
   - Appel √† `/api/create-checkout`
   - Cr√©ation session Stripe avec metadata
   - Redirection vers Stripe

3. **Client paie sur Stripe**
   - Paiement s√©curis√©
   - Infos sauvegard√©es dans metadata

4. **Webhook re√ßoit le paiement**
   - R√©cup√®re metadata (donn√©es du bien)
   - D√©clenche scraping automatiquement
   - G√©n√®re le rapport

5. **Client re√ßoit son rapport**
   - Email de confirmation
   - Rapport de d√©tection (√† venir en Phase 2)

---

## üìä √âTAT ACTUEL (MVP - PHASE 1)

### **‚úÖ FONCTIONNEL :**
- Architecture compl√®te en place
- Webhook d√©clenche le scraping
- Rapport g√©n√©r√© et logg√©
- Donn√©es du bien transmises via Stripe

### **‚è≥ EN COURS (n√©cessite vraie impl√©mentation) :**
- **Scraping r√©el** des plateformes
  - Actuellement : logs des URLs de recherche
  - Phase 2 : vrai scraping avec Puppeteer/API

- **Comparaison de photos**
  - Phase 2 : Computer Vision (OpenAI Vision API)

- **Envoi du rapport par email**
  - Phase 2 : Email avec rapport HTML en pi√®ce jointe

---

## üöÄ PROCHAINES √âTAPES (PHASE 2)

### **1. SCRAPING R√âEL (1 semaine)**

**Option A : Puppeteer (gratuit mais fragile)**
```bash
npm install puppeteer
```
- Navigateur headless
- Scraping direct des sites
- ‚ö†Ô∏è Risque de blocage

**Option B : API de Scraping (recommand√© - 50-100‚Ç¨/mois)**
- **ScraperAPI** : `https://www.scraperapi.com/`
- **Bright Data** : `https://brightdata.com/`
- **Proxies rotatifs**
- **Captcha handling**
- **Plus stable**

**Impl√©mentation :**
```typescript
// Dans /lib/scraping.ts
import puppeteer from 'puppeteer'

async function scrapeAirbnb(property) {
  const browser = await puppeteer.launch()
  const page = await browser.newPage()
  
  await page.goto(`https://airbnb.fr/s/${property.city}`)
  
  // Extraire les r√©sultats
  const listings = await page.$$eval('.listing-card', cards => {
    return cards.map(card => ({
      title: card.querySelector('h3').textContent,
      price: card.querySelector('.price').textContent,
      url: card.querySelector('a').href,
      photos: [...card.querySelectorAll('img')].map(img => img.src)
    }))
  })
  
  await browser.close()
  return listings
}
```

---

### **2. COMPARAISON DE PHOTOS (1 semaine)**

**Option : OpenAI Vision API**
```bash
npm install openai
```

```typescript
import OpenAI from 'openai'

async function comparePhotos(photo1Url, photo2Url) {
  const openai = new OpenAI()
  
  const response = await openai.chat.completions.create({
    model: "gpt-4-vision-preview",
    messages: [{
      role: "user",
      content: [
        { type: "text", text: "Ces deux photos montrent-elles le m√™me bien immobilier ? Score de similarit√© ?" },
        { type: "image_url", image_url: { url: photo1Url } },
        { type: "image_url", image_url: { url: photo2Url } }
      ]
    }]
  })
  
  return response.choices[0].message.content
}
```

---

### **3. G√âN√âRATION PDF + ENVOI EMAIL (3 jours)**

**Installer puppeteer-core pour PDF :**
```bash
npm install puppeteer-core
```

```typescript
import puppeteer from 'puppeteer-core'

async function generatePDF(htmlReport: string) {
  const browser = await puppeteer.launch()
  const page = await browser.newPage()
  
  await page.setContent(htmlReport)
  
  const pdf = await page.pdf({
    format: 'A4',
    printBackground: true
  })
  
  await browser.close()
  return pdf
}

// Envoyer avec Resend
await resend.emails.send({
  from: 'ScanRty <noreply@scanrty.com>',
  to: customerEmail,
  subject: 'Votre rapport de d√©tection ScanRty',
  html: htmlReport,
  attachments: [{
    filename: 'rapport-scanrty.pdf',
    content: pdfBuffer
  }]
})
```

---

### **4. BASE DE DONN√âES (1 semaine)**

**Pour VigilAn (surveillance quotidienne) :**

**Option : Supabase (PostgreSQL gratuit)**
```bash
npm install @supabase/supabase-js
```

**Tables n√©cessaires :**
- `properties` : Biens surveill√©s
- `scans` : Historique des scans
- `detections` : Annonces d√©tect√©es
- `alerts` : Alertes envoy√©es

---

### **5. MONITORING QUOTIDIEN - VigilAn (1 semaine)**

**Avec Vercel Cron Jobs :**

```typescript
// /app/api/cron/daily-scan/route.ts
export async function GET(req: Request) {
  // V√©rifier le secret cron
  if (req.headers.get('authorization') !== `Bearer ${process.env.CRON_SECRET}`) {
    return new Response('Unauthorized', { status: 401 })
  }
  
  // R√©cup√©rer tous les abonnements VigilAn actifs
  const activeProperties = await getActiveVigilAnProperties()
  
  // Scanner chaque bien
  for (const property of activeProperties) {
    await scanProperty(property)
  }
  
  return Response.json({ success: true })
}
```

**Configuration dans `vercel.json` :**
```json
{
  "crons": [{
    "path": "/api/cron/daily-scan",
    "schedule": "0 8 * * *"
  }]
}
```

---

## üí∞ CO√õTS ESTIM√âS (PHASE 2)

### **Services tiers :**
- **ScraperAPI** : 50-100‚Ç¨/mois (10,000 requ√™tes)
- **OpenAI Vision** : ~0.01$/image (n√©gligeable)
- **Supabase** : Gratuit jusqu'√† 500MB
- **Vercel** : Gratuit (Pro si > 100GB bandwidth)
- **Resend** : Gratuit jusqu'√† 3,000 emails/mois

**Total estim√© : 50-150‚Ç¨/mois** selon le volume

---

## üß™ TESTER LE MVP ACTUEL

1. **D√©ploie le code**
2. **Va sur scanrty.com/commander**
3. **Remplis le formulaire complet**
4. **Paie avec une vraie carte** (9‚Ç¨)
5. **Va voir les logs Vercel** ‚Üí Functions ‚Üí `/api/webhooks`
6. **Tu verras :**
   - ‚úÖ Infos du bien r√©cup√©r√©es
   - ‚úÖ Scraping d√©clench√©
   - ‚úÖ URLs de recherche logg√©es
   - ‚úÖ Rapport texte g√©n√©r√©

---

## üìù PROCHAINE D√âCISION

**Tu veux :**

**A. D√©ployer le MVP actuel et tester le flow complet** ? (30 min)

**B. Impl√©menter le scraping r√©el avec Puppeteer** ? (1 journ√©e)

**C. Utiliser une API de scraping (ScraperAPI)** ? (2h + co√ªt)

**D. Commencer par la comparaison de photos** ? (1 journ√©e)

---

**Dis-moi ce que tu pr√©f√®res et on continue ! üöÄ**
